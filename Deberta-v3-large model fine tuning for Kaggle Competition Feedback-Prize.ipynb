{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "from transformers import get_polynomial_decay_schedule_with_warmup, get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup\n",
    "from transformers.tokenization_utils_base import BatchEncoding, PreTrainedTokenizerBase\n",
    "from transformers import DataCollatorWithPadding, DataCollatorForTokenClassification\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW\n",
    "from transformers import TrainingArguments, AutoConfig, AutoModelForTokenClassification\n",
    "\n",
    "from text_unidecode import unidecode\n",
    "from typing import Dict, List, Tuple\n",
    "import codecs\n",
    "from datasets import concatenate_datasets, load_dataset, load_from_disk, Dataset\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"kaggle/working/\"\n",
    "\n",
    "ds_train_path = \"../input/feedback-prize-english-language-learning/train.csv\"\n",
    "\n",
    "ds_test_path = \"../input/feedback-prize-english-language-learning/test.csv\"\n",
    "\n",
    "sample_submission_path = \"../input/feedback-prize-english-language-learning/sample_submission.csv\"\n",
    "\n",
    "save_dir = \"/kaggle/input/deverta-dhankhar-v20/\"\n",
    "\n",
    "deberta_v3_large_offline_path = \"../input/deberta-v3-large/deberta-v3-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    base_dir = base_dir\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    seed = 69\n",
    "    # dataset path \n",
    "    ds_train_path = ds_train_path\n",
    "    ds_test_path = ds_test_path\n",
    "    sample_submission_path = sample_submission_path\n",
    "       \n",
    "    save_dir=save_dir\n",
    "    \n",
    "    #tokenizer params\n",
    "    truncation = True \n",
    "    padding = False #'max_length'\n",
    "    max_length = 512\n",
    "    \n",
    "    # model params\n",
    "    model_name = \"microsoft/deberta-v3-large\"\n",
    "    target_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "    load_from_disk = None\n",
    "    \n",
    "    #training params\n",
    "    learning_rate = 0.1\n",
    "    batch_size = 5\n",
    "    epochs = 5\n",
    "    NFOLDS = 5\n",
    "\n",
    "seed_everything(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df = pd.read_csv(ds_train_path)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = sns.color_palette('Blues')\n",
    "\n",
    "sns.heatmap(train_df.corr(), annot = True, cmap = colormap )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = px.histogram(train_df, x = train_df['full_text'].str.len(), marginal = 'box', title  = 'Histogram of full_text text length', \n",
    "                  color_discrete_sequence = [\"#FFA200\"] )\n",
    "\n",
    "fig.update_layout(bargap = 0.2)\n",
    "\n",
    "fig.update_layout(template = 'plotly_dark', font = dict(family = 'PT Sans', size = 19, color = \"#C4FEFF\"  ) )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "colors = [[\"#00E600\"], [\"#0000E6\"], [\"#E600DF\"], [\"#E6E600\"], [\"#FFFFFF\"], [\"#cd040b\"]]\n",
    "\n",
    "\n",
    "for count, x in enumerate([\"cohesion\", \"syntax\", \"vocabulary\", \"phraseology\", \"grammar\", \"conventions\"]):\n",
    "    fig = px.histogram(train_df, x = x, marginal = 'violin', title = f\" {x} histogram\", color_discrete_sequence = colors[count]  )\n",
    "    \n",
    "    fig.update_layout(bargap = 0.2)\n",
    "    fig.update_layout(template = 'plotly_dark', font = dict(family = 'PT Sans', size = 19, color = \"#C4FEFF\" ) )\n",
    "    \n",
    "    fig.show() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n",
    "    return error.object[error.start : error.end].encode('utf-8'), error.end\n",
    "\n",
    "\n",
    "# Read further - https://docs.python.org/3/library/codecs.html\n",
    "\n",
    "def replace_decoding_with_cp1252(error : UnicodeError ) -> Tuple[str, int]:\n",
    "    return error.object[error.start : error.end].decode('cp1252'), error.end\n",
    "\n",
    "# Register the encoding and decoding error handlers for `utf-8` and `cp1252`.\n",
    "codecs.register_error('replace_encoding_with_utf8', replace_encoding_with_utf8)\n",
    "codecs.register_error('replace_decoding_with_cp1252', replace_decoding_with_cp1252)\n",
    "\n",
    "\n",
    "def resolve_encodings_and_normalize(text: str) -> str:\n",
    "    text = (\n",
    "        text.encode('raw_unicode_escape')\n",
    "        .decode('utf-8', errors = 'replace_decoding_with_cp1252' )\n",
    "        .decode('cp1252', errors = 'replace_encoding_with_utf8' )\n",
    "        .decode('utf-8', errors = 'replace_decoding_with_cp1252' )\n",
    "    )\n",
    "    \n",
    "    text = unidecode(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(config.ds_train_path)\n",
    "df_test = pd.read_csv(config.ds_test_path)\n",
    "df_ss = pd.read_csv(config.sample_submission_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train['full_text'] = df_train['full_text'].apply(resolve_encodings_and_normalize)\n",
    "\n",
    "df_test['full_text'] = df_test['full_text'].apply(resolve_encodings_and_normalize)\n",
    "\n",
    "df_test[config.target_cols] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(deberta_v3_large_offline_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize(df):\n",
    "    \"\"\"\n",
    "    Tokenize the text data using a tokenizer.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame containing the text data.\n",
    "\n",
    "    Returns:\n",
    "        dict: Tokenized data containing input IDs, attention masks,\n",
    "              labels, and length.\n",
    "\n",
    "    \"\"\"\n",
    "    text = df['full_text']\n",
    "    \n",
    "    tokenized = tokenizer(text,\n",
    "                          padding = config.padding,\n",
    "                          truncation = True,\n",
    "                          max_length = config.max_length\n",
    "                          add_special_tokens = True\n",
    "                         )\n",
    "    tokenized['labels'] = [df[i] for i in config.target_cols ]\n",
    "    tokenized['length'] = len(tokenized['input_ids'])\n",
    "    \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.load_from_disk is None:\n",
    "    \n",
    "    ds = Dataset.from_pandas(df_test)\n",
    "    \n",
    "    ds = ds.map(\n",
    "        tokenize,\n",
    "        batched=False,\n",
    "        num_proc=4,\n",
    "        desc='Tokenizing'\n",
    "    )\n",
    "    \n",
    "    \n",
    "ds.save_to_disk(f\" {config.base_dir}data.dataset\")\n",
    "\n",
    "with open(f\"{config.base_dir}_pkl\", \"wb\" ) as fp:\n",
    "    pickle.dump(df_test, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Mean pooling module for NLP tasks.\n",
    "\n",
    "    This module calculates the mean of the hidden state vectors\n",
    "    based on the attention mask.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the mean pooling module.\n",
    "\n",
    "        Args:\n",
    "            last_hidden_state (torch.Tensor): Last hidden state from the transformer.\n",
    "            attention_mask (torch.Tensor): Attention mask indicating valid tokens.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Mean-pooled embeddings.\n",
    "\n",
    "        \"\"\"\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1 )\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min = 1e-9 )\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DebartaLargeModel(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    This class is a PyTorch Lightning Module that uses the DeBERTa model for NLP tasks.\n",
    "\n",
    "    Args:\n",
    "        config: A dictionary containing the configuration parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.cfg = config\n",
    "\n",
    "        self.model_config = AutoConfig.from_pretrained(deberta_v3_large_offline_path + 'config.json')\n",
    "        self.model_config.update(\n",
    "            {\n",
    "                \"output_hidden_states\": True,\n",
    "                \"hidden_dropout_prob\": 0.,\n",
    "                \"add_pooling_layer\": False,\n",
    "                \"attention_probs_dropout_prob\":0,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        self.transformers_model = AutoModel.from_pretrained(deberta_v3_large_offline_path, config=self.model_config)\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(self.transformers_model.config.hidden_size * 2)  # Concat of mean and max pooling\n",
    "        \n",
    "        self.output = nn.Linear(self.transformers_model.config.hidden_size * 2, len(self.cfg.target_cols))   #  + num_external_features\n",
    "\n",
    "        self.hidden_layer_weights = nn.Parameter(torch.zeros(self.model_config.num_hidden_layers).view(-1, 1, 1, 1))\n",
    "        \n",
    "        ''' Dropout layers \n",
    "        `nn.ModuleList` is just like a Python list. It was designed to store any desired number of nn.Module’s. \n",
    "        It may be useful, for instance, if you want to design a neural network whose number of layers is passed as input:\n",
    "\n",
    "        So why use ModuleList instead of a normal python list? Ans - If you use a plain python list, \n",
    "        the parameters won’t be registered properly and you can’t pass them to your optimizer using model.parameters().\n",
    "        '''\n",
    "\n",
    "        self.dropouts = nn.ModuleList([\n",
    "                nn.Dropout(0.1*i) for i in range(5)\n",
    "            ])\n",
    "        self.loss_function = nn.SmoothL1Loss(reduction='mean') \n",
    "        \n",
    "    def forward(self, input_ids, attention_mask,train):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            input_ids: Tensor of input ids.\n",
    "            attention_mask: Tensor of attention masks.\n",
    "            train: Boolean indicating if the model is in training mode.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of logits and _.\n",
    "        \"\"\"\n",
    "        \n",
    "        output_backbone = self.transformers_model(input_ids, attention_mask = attention_mask)#[0]\n",
    "        \n",
    "        hidden_states = output_backbone.hidden_states\n",
    "        \n",
    "        # Mean/max pooling (over hidden layers), concatenate with pooler\n",
    "        hidden_states = torch.stack(tuple(hidden_states[-i-1] for i in range(len(hidden_states) - 1)), dim = 0)\n",
    "        \n",
    "        layer_weight = F.softmax(self.hidden_layer_weights, dim = 0)\n",
    "        \n",
    "        out_mean = torch.sum(hidden_states * layer_weight, dim = 0)\n",
    "        \n",
    "        out_max, _ = torch.max(hidden_states, dim = 0)\n",
    "        \n",
    "        # concatenate with pooler, i.e. Concat of mean and max pooling\n",
    "        output_backbone = torch.cat((out_mean, out_max), dim = -1)\n",
    "        \n",
    "        output_backbone = self.layer_norm(output_backbone)[:,0,:]\n",
    "\n",
    "        # print(output_backbone.shape) # torch.Size([3, 2048])\n",
    "        \n",
    "        # Multiple dropout\n",
    "        for i, dropout in enumerate(self.dropouts):\n",
    "            if i == 0:\n",
    "                logits = self.output(output_backbone)\n",
    "            else:\n",
    "                logits += self.output(output_backbone)\n",
    "        \n",
    "        logits /= len(self.dropouts)\n",
    "        return (logits, _)\n",
    "    \n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        \"\"\"\n",
    "        Returns the training dataloader.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader for the training data.\n",
    "        \"\"\"\n",
    "        return self._train_dataloader \n",
    "    \n",
    "    def validation_dataloader(self):\n",
    "        \"\"\"\n",
    "        Returns the validation dataloader.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader for the validation data.\n",
    "        \"\"\"\n",
    "        return self._validation_dataloader\n",
    "\n",
    "    #  we define a helper function to differentiate the parameters that should receive weight decay.\n",
    "    def get_optimizer_params(self, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "        \"\"\"\n",
    "        Helper function to get the parameters for the optimizer.\n",
    "\n",
    "        Args:\n",
    "            encoder_lr: Learning rate for the encoder.\n",
    "            decoder_lr: Learning rate for the decoder.\n",
    "            weight_decay: Weight decay rate.\n",
    "\n",
    "        Returns:\n",
    "            List of parameters for the optimizer.\n",
    "        \"\"\"\n",
    "        \n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        \n",
    "        no_decay_list = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        # In general, biases and LayerNorm weights are not subject to weight decay:\n",
    "        \n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in self.transformers_model.named_parameters() if not any(nd in n for nd in no_decay_list)],\n",
    "             'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "            \n",
    "            {'params': [p for n, p in self.transformers_model.named_parameters() if any(nd in n for nd in no_decay_list)],\n",
    "             'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "            \n",
    "            {'params': [p for n, p in self.named_parameters() if \"transformers_model\" not in n],\n",
    "             'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configures the optimizer for the training process.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing the optimizer and the learning rate scheduler.\n",
    "        \"\"\"\n",
    "        optimizer = AdamW(self.parameters(), lr = config.learning_rate)\n",
    "\n",
    "        epoch_steps = self.cfg.data_length\n",
    "        batch_size = self.cfg.batch_size\n",
    "\n",
    "        # The idea of warmup_steps is to use some warmup steps to increase the learning rate \n",
    "        # up to a certain point and then use your normal learning rate decay afterwards. \n",
    "        warmup_steps = 0.09 * epoch_steps // batch_size\n",
    "        training_steps = self.cfg.epochs * epoch_steps // batch_size\n",
    "        # scheduler = get_linear_schedule_with_warmup(optimizer,warmup_steps,training_steps,-1)\n",
    "        scheduler = get_polynomial_decay_schedule_with_warmup(optimizer, warmup_steps, training_steps, lr_end=7e-7, power=3.0)\n",
    "\n",
    "        lr_scheduler_config = {\n",
    "                'scheduler': scheduler,\n",
    "                'interval': 'step',\n",
    "                'frequency': 1,\n",
    "            }\n",
    "\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': lr_scheduler_config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_loader, model):\n",
    "    \"\"\"\n",
    "    Generates predictions using a trained model on the provided data loader.\n",
    "\n",
    "    Args:\n",
    "        data_loader (DataLoader): DataLoader containing the input data.\n",
    "        model (torch.nn.Module): Trained model to use for prediction.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Array of predicted outputs.\n",
    "\n",
    "    \"\"\"\n",
    "    model.to(config.device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    for batch in tqdm(data_loader):\n",
    "        with torch.no_grad():\n",
    "            inputs = { key:val.reshape(val.shape[0], -1).to(config.device) for key, val in batch.items() }\n",
    "            outputs = model(input_ids = inputs['input_ids'], attention_mask = inputs['attention_mask'], train = False )[0]\n",
    "            \n",
    "        predictions.extend(outputs.detach().cpu().numpy())\n",
    "        \n",
    "    predictions = np.vstack(predictions)\n",
    "    \n",
    "    return predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorWithPadding(\n",
    "    tokenizer = tokenizer, pad_to_multiple_of = 16, padding = 'longest'\n",
    ")\n",
    "\n",
    "for fold in range(config.NFOLDS):\n",
    "    train_ds_list = []\n",
    "    \n",
    "    print(f\"########### FOLD RUNNING {fold} ################ \")\n",
    "    \n",
    "    keep_cols = {\"input_ids\", \"attention_mask\", \"labels\"}\n",
    "    \n",
    "    test_ds = load_from_disk(f'{config.base_dir}data.dataset').sort('length')\n",
    "    \n",
    "    test_ds = test_ds.remove_columns([ c for c in test_ds.column_names if c not in keep_cols ])\n",
    "    \n",
    "    config.data_length = len(test_ds)\n",
    "    \n",
    "    test_dataloader = DataLoader(test_ds, batch_size=config.batch_size, shuffle=False, num_workers=2, pin_memory=True, collate_fn=collator )\n",
    "    \n",
    "    \n",
    "    model = DebertaLargeModel.load_from_checkpoint(f'{config.save_dir}microsoft/deberta-v3-large_{fold}.ckpt', train_dataloader = None, validation_dataloader = None, config = config  )\n",
    "    \n",
    "    preds = predict(test_dataloader, model )\n",
    "    \n",
    "    if fold == 0:\n",
    "        final_preds = preds * (1/config.NFOLDS)\n",
    "        \n",
    "    else:\n",
    "        final_preds += preds * (1/config.NFOLDS)\n",
    "        \n",
    "    del model, test_dataloader, test_ds\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = []\n",
    "\n",
    "test_ds = load_from_disk(f'{config.base_dir}data.dataset').sor('length')\n",
    "\n",
    "for i in test_ds:\n",
    "    test_ids.append(i['text_id'])\n",
    "    \n",
    "print(final_preds.shape)\n",
    "\n",
    "sub_df = pd.DataFrame(test_ids, columns = {'text_id'} )\n",
    "\n",
    "sub_df[config.target_cols] = final_preds.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Itachi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
