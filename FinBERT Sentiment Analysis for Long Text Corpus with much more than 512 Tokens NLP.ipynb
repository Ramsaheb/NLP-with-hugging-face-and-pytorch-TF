{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('ProsusAI/finbert')\n",
    "model = BertForSequenceClassification.from_pretrained('ProsusAI/finbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(tokens):\n",
    "    outputs = model(**tokens)\n",
    "    probabilities = torch.nn.functional.softmax(outputs[0], dim=-1)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = '''\n",
    "1. Higher levels of retail participation in crypto than traditional commodity markets pose unique challenges for regulators.\n",
    "One in five Americans report having traded cryptocurrency, and polls suggest crypto trading is more common among younger adults, men, and racial minorities. This is quite different from other financial instruments regulated by the CFTC, Benham noted. “You’re going to have more vulnerable investors… It’s incumbent on us to educate, to inform, to disclose risks involved.”\n",
    "\n",
    "Michael Piwowar, a former Securities and Exchange Commissioner and now executive director of the Milken Institute Center for Financial Markets, linked increased Congressional attention to growth in retail crypto: “If you got one in five households that have interacted with crypto… [members of Congress] are going to start hearing it from their constituents.” Legislation to regulate digital assets has been introduced by Senators Lummis and Gillibrand, Stabenow and Boozman, and Toomey, as well as Representative Gottheimer. The Treasury is actively negotiating bipartisan stablecoin legislation with House Financial Services Committee Chair Waters and Ranking Member McHenry. Benham said that stablecoins, digital currency meant to always be equal to one dollar, are more of a “payment mechanism” and thus should be regulated by prudential banking regulators.\n",
    "\n",
    "Digital asset regulation may require addressing crypto exchanges and digital wallets. American University Law Professor Hilary Allen noted that the stablecoin legislation under discussion does not, saying, “That is a gaping hole… Almost every major stablecoin… is affiliated with an exchange that profits from trading in that stablecoin.” Mark Wetjen, a former CFTC commissioner and current head of policy and regulatory strategy for FTX (one of the largest crypto exchanges), agreed: “The exchanges are the gateways to the entire crypto space, and so oversight of them is probably most important.” He pushed back that there was no current regulation, noting the requirement for state level licenses, such as New York’s Bitlicense: “If you want to list derivatives on bitcoin, for example, you need a license… so it may not be as dire a situation.”\n",
    "\n",
    "2. Crypto challenges traditional regulatory distinction between securities and commodities.\n",
    "Traditionally, the SEC regulates securities while the CFTC regulates commodities and derivatives. Whether crypto is a security or commodity remains unclear, as various subcomponents of the crypto ecosystem challenge existing regulatory divisions. For instance, the SEC recently argued  that nine different crypto tokens were securities in an insider trading case while a federal judge ruled that virtual currency like Bitcoin constitutes a commodity.\n",
    "\n",
    "Benham called on Congress to provide clarity on which of the hundreds – if not thousands – of coins in existence are securities versus commodities: “Ultimately, we’d like to see law drawing lines.” Piwowar said the lack of clarity creates unwelcome delays as many crypto-related applications before the SEC are “not getting answers” on whether their products represent securities. The result is that some crypto firms are “going outside the United States” to locate their business. Allen cautioned, though, that Congressional action could also constitute an indication that the government supports crypto. She warned against letting crypto into the regulated sphere for fear of giving it “implicit guarantees.”\n",
    "\n",
    "A solution to the regulatory turf battle could be merging the SEC and CFTC, which Piwowar endorsed, as have many others. Congress, however, has shown little appetite to do so given the different Congressional committee jurisdictions involved.\n",
    "\n",
    "3. CFTC will restructure to better protect consumers and more effectively regulate markets.\n",
    "Benham announced several changes at the CFTC during the Brookings event. First, LabCFTC will become the Office of Technology Innovation, reporting directly to the Chairman’s office. Behnam justified this by stating, “We are past the incubator stage, and digital assets and decentralized financial technologies have outgrown their sandboxes.” Second, CFTC’s Office of Customer Education and Outreach will be realigned within the Office of Public Affairs, which Behnam said would “leverage resources and a broader understanding of the issues facing the general public towards addressing the most critical needs in the most vulnerable communities.” Restructuring within a regulator may appear a bureaucratic shuffle but can reflect changes in internal power, agency focus, and prioritization. Directly reporting to the chair increases an office’s authority and prestige.\n",
    "\n",
    "4. Is crypto a passing fad (or worse, a bubble that threatens financial markets)?\n",
    "Allen argued that crypto is “purposely less efficient and more complicated than a more centralized system,” and does not have any societal value. FTX’s Wetjen disagreed: “The difference here with blockchain as the underpinning means by which you can transfer value is that there are absolutely no gates.” Piwowar broadly agreed with Wetjen that “We’re going to have the new generation of Amazons and Googles come out of this stuff,” but cautioned that while he was at the SEC, “Nine out of ten [crypto applications] were outright fraud, and then out of the one out of ten, nine out of ten of those were probably fraud.” Since January 2021, over 46,000 people have collectively lost over $1 billion to scams involving crypto.\n",
    "\n",
    "Everyone wants to avoid a repeat of the 2008 global financial crisis. To do so, regulators have focused on avoiding and mitigating “systemic risk” to the financial system. Asked if he sees a “clear and present danger to the existing economic system,” Benham said he did not, pointing out that crypto is not sufficiently interconnected to pose systemic risk. He noted the decrease in crypto values over the past several months did not cause ripples in the financial system or the broader economy. Piwowar turned the question of systemic risk back onto the actions of financial regulators asking: “What is systemic risk?  It’s the risk that a federal policymaker is going to bail out a bank, either directly or indirectly.” Allen agreed that bailing out crypto would be a mistake quipping: “If anything should be able to fail, it should be crypto, which isn’t… funding productive economic capacity.”\n",
    "\n",
    "Allen also noted the similarity in arguments centered on American global competitiveness which promoted lax regulation for derivatives: “It’s almost identical to the rhetoric we saw around swaps in the 1990s.” Credit default swaps, like crypto now, faced loose regulation and ultimately helped fuel the subprime mortgage crisis. Behnam noted that one of 2008’s biggest lessons was the need for the CFTC to promote market transparency in the “OTC [over-the-counter] derivative space.” Crypto proponents point to the underlying technology as being inherently more transparent, while critics point to the lack of understanding of aspects of the market, such as what assets back stablecoins like Tether.\n",
    "\n",
    "5. Does crypto increase financial inclusion?\n",
    "Cryptocurrency proponents frequently cite financial inclusion as a major benefit linking the higher usage of youth and communities of color who have higher rates of being unbanked or underbanked by traditional finance. Allen cautioned against “predatory inclusion” arguing that, “Because there’s no productive capacity behind them, their value derives from finding someone else to buy them from you.” Wetjen responded, blending his experience serving as a CFTC Commissioner with his time in the crypto industry: “From my own experience… at the CFTC, there’s plenty of authority that’s already in place for the agency to… be pretty thoughtful and relatively prescriptive, even in terms of what actually should be disclosed to, particularly, retail investors, or users of a platform such as FTX.” He argued that the right policy is “giving people the opportunity to be involved and invest in the space that they like but making sure that it’s done with the right safeguards.”\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1651 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1015, 1012, 3020, 3798, 1997, 7027, 6577, 1999, 19888, 2080, 2084, 3151, 19502, 6089, 13382, 4310, 7860, 2005, 25644, 1012, 2028, 1999, 2274, 4841, 3189, 2383, 7007, 19888, 10085, 3126, 7389, 5666, 1010, 1998, 14592, 6592, 19888, 2080, 6202, 2003, 2062, 2691, 2426, 3920, 6001, 1010, 2273, 1010, 1998, 5762, 14302, 1012, 2023, 2003, 3243, 2367, 2013, 2060, 3361, 5693, 12222, 2011, 1996, 12935, 13535, 1010, 3841, 3511, 3264, 1012, 1523, 2017, 1521, 2128, 2183, 2000, 2031, 2062, 8211, 9387, 1529, 2009, 1521, 1055, 7703, 2006, 2149, 2000, 16957, 1010, 2000, 12367, 1010, 2000, 26056, 10831, 2920, 1012, 1524, 2745, 14255, 12155, 9028, 1010, 1037, 2280, 12012, 1998, 3863, 5849, 1998, 2085, 3237, 2472, 1997, 1996, 6501, 2368, 2820, 2415, 2005, 3361, 6089, 1010, 5799, 3445, 7740, 3086, 2000, 3930, 1999, 7027, 19888, 2080, 1024, 1523, 2065, 2017, 2288, 2028, 1999, 2274, 3911, 2008, 2031, 11835, 2098, 2007, 19888, 2080, 1529, 1031, 2372, 1997, 3519, 1033, 2024, 2183, 2000, 2707, 4994, 2009, 2013, 2037, 24355, 1012, 1524, 6094, 2000, 15176, 3617, 7045, 2038, 2042, 3107, 2011, 10153, 11320, 7382, 2483, 1998, 12267, 12322, 13033, 1010, 17079, 16515, 2860, 1998, 22017, 24340, 1010, 1998, 2205, 4168, 2100, 1010, 2004, 2092, 2004, 4387, 2288, 10760, 14428, 2099, 1012, 1996, 9837, 2003, 8851, 18875, 12170, 26053, 6540, 3597, 2378, 6094, 2007, 2160, 3361, 2578, 2837, 3242, 5380, 1998, 5464, 2266, 11338, 10222, 2854, 1012, 3841, 3511, 2056, 2008, 6540, 3597, 7076, 1010, 3617, 9598, 3214, 2000, 2467, 2022, 5020, 2000, 2028, 7922, 1010, 2024, 2062, 1997, 1037, 1523, 7909, 7337, 1524, 1998, 2947, 2323, 2022, 12222, 2011, 10975, 12672, 19909, 8169, 25644, 1012, 3617, 11412, 7816, 2089, 5478, 12786, 19888, 2080, 15800, 1998, 3617, 15882, 2015, 1012, 2137, 2118, 2375, 2934, 22744, 5297, 3264, 2008, 1996, 6540, 3597, 2378, 6094, 2104, 6594, 2515, 2025, 1010, 3038, 1010, 1523, 2008, 2003, 1037, 21226, 4920, 1529, 2471, 2296, 2350, 6540, 3597, 2378, 1529, 2003, 6989, 2007, 2019, 3863, 2008, 11372, 2013, 6202, 1999, 2008, 6540, 3597, 2378, 1012, 1524, 2928, 4954, 6460, 2078, 1010, 1037, 2280, 12935, 13535, 5849, 1998, 2783, 2132, 1997, 3343, 1998, 10738, 5656, 2005, 3027, 2595, 1006, 2028, 1997, 1996, 2922, 19888, 2080, 15800, 1007, 1010, 3530, 1024, 1523, 1996, 15800, 2024, 1996, 11909, 2015, 2000, 1996, 2972, 19888, 2080, 2686, 1010, 1998, 2061, 15709, 1997, 2068, 2003, 2763, 2087, 2590, 1012, 1524, 2002, 3724, 2067, 2008, 2045, 2001, 2053, 2783, 7816, 1010, 9073, 1996, 9095, 2005, 2110, 2504, 15943, 1010, 2107, 2004, 2047, 2259, 1521, 1055, 2978, 13231, 12325, 1024, 1523, 2065, 2017, 2215, 2000, 2862, 16942, 2006, 2978, 3597, 2378, 1010, 2005, 2742, 1010, 2017, 2342, 1037, 6105, 1529, 2061, 2009, 2089, 2025, 2022, 2004, 18704, 1037, 3663, 1012, 1524, 1016, 1012, 19888, 2080, 7860, 3151, 10738, 7835, 2090, 12012, 1998, 21955, 1012, 6964, 1010, 1996, 10819, 26773, 12012, 2096, 1996, 12935, 13535, 26773, 21955, 1998, 16942, 1012, 3251, 19888, 2080, 2003, 1037, 3036, 2030, 19502, 3464, 10599, 1010, 2004, 2536, 4942, 9006, 29513, 7666, 1997, 1996, 19888, 2080, 16927, 4119, 4493, 10738, 5908, 1012, 2005, 6013, 1010, 1996, 10819, 3728, 5275, 2008, 3157, 2367, 19888, 2080, 19204, 2015, 2020, 12012, 1999, 2019, 25297, 6202, 2553, 2096, 1037, 2976, 3648, 5451, 2008, 7484, 9598, 2066, 2978, 3597, 2378, 17367, 1037, 19502, 1012, 3841, 3511, 2170, 2006, 3519, 2000, 3073, 15563, 2006, 2029, 1997, 1996, 5606, 1516, 2065, 2025, 5190, 1516, 1997, 7824, 1999, 4598, 2024, 12012, 6431, 21955, 1024, 1523, 4821, 1010, 2057, 1521, 1040, 2066, 2000, 2156, 2375, 5059, 3210, 1012, 1524, 14255, 12155, 9028, 2056, 1996, 3768, 1997, 15563, 9005, 4895, 8545, 22499, 4168, 14350, 2004, 2116, 19888, 2080, 1011, 3141, 5097, 2077, 1996, 10819, 2024, 1523, 2025, 2893, 6998, 1524, 2006, 3251, 2037, 3688, 5050, 12012, 1012, 1996, 2765, 2003, 2008, 2070, 19888, 2080, 9786, 2024, 1523, 2183, 2648, 1996, 2142, 2163, 1524, 2000, 12453, 2037, 2449, 1012, 5297, 14046, 2098, 1010, 2295, 1010, 2008, 7740, 2895, 2071, 2036, 12346, 2019, 12407, 2008, 1996, 2231, 6753, 19888, 2080, 1012, 2016, 7420, 2114, 5599, 19888, 2080, 2046, 1996, 12222, 10336, 2005, 3571, 1997, 3228, 2009, 1523, 24655, 21586, 1012, 1524, 1037, 5576, 2000, 1996, 10738, 14585, 2645, 2071, 2022, 16468, 1996, 10819, 1998, 12935, 13535, 1010, 2029, 14255, 12155, 9028, 11763, 1010, 2004, 2031, 2116, 2500, 1012, 3519, 1010, 2174, 1010, 2038, 3491, 2210, 18923, 2000, 2079, 2061, 2445, 1996, 2367, 7740, 2837, 17370, 2920, 1012, 1017, 1012, 12935, 13535, 2097, 2717, 6820, 14890, 2000, 2488, 4047, 10390, 1998, 2062, 6464, 15176, 6089, 1012, 3841, 3511, 2623, 2195, 3431, 2012, 1996, 12935, 13535, 2076, 1996, 9566, 8613, 2724, 1012, 2034, 1010, 6845, 2278, 6199, 2278, 2097, 2468, 1996, 2436, 1997, 2974, 8144, 1010, 7316, 3495, 2000, 1996, 3472, 1521, 1055, 2436, 1012, 2022, 7295, 3286, 15123, 2023, 2011, 5517, 1010, 1523, 2057, 2024, 2627, 1996, 4297, 19761, 4263, 2754, 1010, 1998, 3617, 7045, 1998, 11519, 7941, 3550, 3361, 6786, 2031, 2041, 16523, 12384, 2037, 5472, 8758, 2229, 1012, 1524, 2117, 1010, 12935, 13535, 1521, 1055, 2436, 1997, 8013, 2495, 1998, 15641, 2097, 2022, 2613, 23773, 2098, 2306, 1996, 2436, 1997, 2270, 3821, 1010, 2029, 2022, 7295, 3286, 2056, 2052, 1523, 21155, 4219, 1998, 1037, 12368, 4824, 1997, 1996, 3314, 5307, 1996, 2236, 2270, 2875, 12786, 1996, 2087, 4187, 3791, 1999, 1996, 2087, 8211, 4279, 1012, 1524, 18322, 2306, 1037, 21618, 2089, 3711, 1037, 4879, 17510, 23046, 2021, 2064, 8339, 3431, 1999, 4722, 2373, 1010, 4034, 3579, 1010, 1998, 3188, 25090, 9276, 1012, 3495, 7316, 2000, 1996, 3242, 7457, 2019, 2436, 1521, 1055, 3691, 1998, 14653, 1012, 1018, 1012, 2003, 19888, 2080, 1037, 4458, 6904, 2094, 1006, 2030, 4788, 1010, 1037, 11957, 2008, 17016, 3361, 6089, 1007, 1029, 5297, 5275, 2008, 19888, 2080, 2003, 1523, 24680, 2625, 8114, 1998, 2062, 8552, 2084, 1037, 2062, 22493, 2291, 1010, 1524, 1998, 2515, 2025, 2031, 2151, 23382, 3643, 1012, 3027, 2595, 1521, 1055, 4954, 6460, 2078, 18335, 1024, 1523, 1996, 4489, 2182, 2007, 3796, 24925, 2078, 2004, 1996, 2104, 8091, 5582, 2965, 2011, 2029, 2017, 2064, 4651, 3643, 2003, 2008, 2045, 2024, 7078, 2053, 6733, 1012, 1524, 14255, 12155, 9028, 13644, 3530, 2007, 4954, 6460, 2078, 2008, 1523, 2057, 1521, 2128, 2183, 2000, 2031, 1996, 2047, 4245, 1997, 9733, 2015, 1998, 8224, 2015, 2272, 2041, 1997, 2023, 4933, 1010, 1524, 2021, 14046, 2098, 2008, 2096, 2002, 2001, 2012, 1996, 10819, 1010, 1523, 3157, 2041, 1997, 2702, 1031, 19888, 2080, 5097, 1033, 2020, 13848, 9861, 1010, 1998, 2059, 2041, 1997, 1996, 2028, 2041, 1997, 2702, 1010, 3157, 2041, 1997, 2702, 1997, 2216, 2020, 2763, 9861, 1012, 1524, 2144, 2254, 25682, 1010, 2058, 4805, 1010, 2199, 2111, 2031, 13643, 2439, 2058, 1002, 1015, 4551, 2000, 8040, 13596, 5994, 19888, 2080, 1012, 3071, 4122, 2000, 4468, 1037, 9377, 1997, 1996, 2263, 3795, 3361, 5325, 1012, 2000, 2079, 2061, 1010, 25644, 2031, 4208, 2006, 9992, 1998, 10210, 13340, 3436, 1523, 22575, 3891, 1524, 2000, 1996, 3361, 2291, 1012, 2356, 2065, 2002, 5927, 1037, 1523, 3154, 1998, 2556, 5473, 2000, 1996, 4493, 3171, 2291, 1010, 1524, 3841, 3511, 2056, 2002, 2106, 2025, 1010, 7302, 2041, 2008, 19888, 2080, 2003, 2025, 12949, 6970, 24230, 2000, 13382, 22575, 3891, 1012, 2002, 3264, 1996, 9885, 1999, 19888, 2080, 5300, 2058, 1996, 2627, 2195, 2706, 2106, 2025, 3426, 24644, 2015, 1999, 1996, 3361, 2291, 2030, 1996, 12368, 4610, 1012, 14255, 12155, 9028, 2357, 1996, 3160, 1997, 22575, 3891, 2067, 3031, 1996, 4506, 1997, 3361, 25644, 4851, 1024, 1523, 2054, 2003, 22575, 3891, 1029, 2009, 1521, 1055, 1996, 3891, 2008, 1037, 2976, 3343, 8571, 2003, 2183, 2000, 15358, 2041, 1037, 2924, 1010, 2593, 3495, 2030, 17351, 1012, 1524, 5297, 3530, 2008, 15358, 2075, 2041, 19888, 2080, 2052, 2022, 1037, 6707, 21864, 14853, 1024, 1523, 2065, 2505, 2323, 2022, 2583, 2000, 8246, 1010, 2009, 2323, 2022, 19888, 2080, 1010, 2029, 3475, 1521, 1056, 1529, 4804, 13318, 3171, 3977, 1012, 1524, 5297, 2036, 3264, 1996, 14402, 1999, 9918, 8857, 2006, 2137, 3795, 6975, 2791, 2029, 3755, 27327, 7816, 2005, 16942, 1024, 1523, 2009, 1521, 1055, 2471, 7235, 2000, 1996, 17871, 2057, 2387, 2105, 19948, 2015, 1999, 1996, 4134, 1012, 1524, 4923, 12398, 19948, 2015, 1010, 2066, 19888, 2080, 2085, 1010, 4320, 6065, 7816, 1998, 4821, 3271, 4762, 1996, 4942, 18098, 14428, 14344, 5325, 1012, 2022, 7295, 3286, 3264, 2008, 2028, 1997, 2263, 1521, 1055, 5221, 8220, 2001, 1996, 2342, 2005, 1996, 12935, 13535, 2000, 5326, 3006, 16987, 1999, 1996, 1523, 27178, 2278, 1031, 2058, 1011, 1996, 1011, 4675, 1033, 13819, 2686, 1012, 1524, 19888, 2080, 20401, 2391, 2000, 1996, 10318, 2974, 2004, 2108, 26096, 2062, 13338, 1010, 2096, 4401, 2391, 2000, 1996, 3768, 1997, 4824, 1997, 5919, 1997, 1996, 3006, 1010, 2107, 2004, 2054, 7045, 2067, 6540, 3597, 7076, 2066, 8915, 12399, 1012, 1019, 1012, 2515, 19888, 2080, 3623, 3361, 10502, 1029, 19888, 10085, 3126, 7389, 5666, 20401, 4703, 21893, 3361, 10502, 2004, 1037, 2350, 5770, 11383, 1996, 3020, 8192, 1997, 3360, 1998, 4279, 1997, 3609, 2040, 2031, 3020, 6165, 1997, 2108, 4895, 9299, 2098, 2030, 2104, 9299, 2098, 2011, 3151, 5446, 1012, 5297, 14046, 2098, 2114, 1523, 21659, 10502, 1524, 9177, 2008, 1010, 1523, 2138, 2045, 1521, 1055, 2053, 13318, 3977, 2369, 2068, 1010, 2037, 3643, 12153, 2013, 4531, 2619, 2842, 2000, 4965, 2068, 2013, 2017, 1012, 1524, 4954, 6460, 2078, 5838, 1010, 23293, 2010, 3325, 3529, 2004, 1037, 12935, 13535, 5849, 2007, 2010, 2051, 1999, 1996, 19888, 2080, 3068, 1024, 1523, 2013, 2026, 2219, 3325, 1529, 2012, 1996, 12935, 13535, 1010, 2045, 1521, 1055, 7564, 1997, 3691, 2008, 1521, 1055, 2525, 1999, 2173, 2005, 1996, 4034, 2000, 1529, 2022, 3492, 16465, 1998, 4659, 3653, 22483, 3512, 1010, 2130, 1999, 3408, 1997, 2054, 2941, 2323, 2022, 21362, 2000, 1010, 3391, 1010, 7027, 9387, 1010, 2030, 5198, 1997, 1037, 4132, 2107, 2004, 3027, 2595, 1012, 1524, 2002, 5275, 2008, 1996, 2157, 3343, 2003, 1523, 3228, 2111, 1996, 4495, 2000, 2022, 2920, 1998, 15697, 1999, 1996, 2686, 2008, 2027, 2066, 2021, 2437, 2469, 2008, 2009, 1521, 1055, 2589, 2007, 1996, 2157, 28805, 2015, 1012, 1524], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.encode_plus(txt, add_special_tokens=False)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1651\n",
      "1651\n",
      "1651\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens.input_ids))\n",
    "print(len(tokens.token_type_ids))\n",
    "print(len(tokens.attention_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokens['input_ids']\n",
    "token_type_ids = tokens['token_type_ids']\n",
    "attention_mask = tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1015, 1012, 3020, 3798, 1997, 7027, 6577, 1999, 19888, 2080]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[: 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start = 0\n",
      "end = 512\n",
      "start = 512\n",
      "end = 1024\n",
      "start = 1024\n",
      "end = 1536\n",
      "start = 1536\n",
      "end = 1651\n"
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "window_len = 512\n",
    "total_len = len(input_ids)\n",
    "loop = True\n",
    "\n",
    "while loop:\n",
    "    end = start + window_len\n",
    "    if end >= total_len:\n",
    "        loop = False\n",
    "        end = total_len\n",
    "    \n",
    "    print(f'start = {start}')\n",
    "    print(f'end = {end}')\n",
    "    start = end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text_to_window_size_and_predict_proba(attention_mask, input_ids, total_len):\n",
    "    proba_list = []\n",
    "    start = 0\n",
    "    window_len = 510\n",
    "\n",
    "    loop = True\n",
    "\n",
    "    while loop:\n",
    "        end = start + window_len\n",
    "        if end >= total_len:\n",
    "            loop = False\n",
    "            end = total_len\n",
    "\n",
    "        #1 ==> define the text chunk\n",
    "        input_ids_chunk = input_ids[start:end]\n",
    "        attention_mask_chunk = attention_mask[start:end]\n",
    "\n",
    "        #2 ==> append cls and sep\n",
    "        input_ids_chunk = [101] + input_ids_chunk + [102]\n",
    "        attention_mask_chunk = [1] + attention_mask_chunk + [1]\n",
    "\n",
    "        #3 ==> convert dict to pytorch tensor\n",
    "        input_dict = {\n",
    "            'input_ids' : torch.Tensor([input_ids_chunk]).long(),\n",
    "            'attention_mask' : torch.Tensor([attention_mask_chunk]).int()\n",
    "        }\n",
    "\n",
    "        outputs = model(**input_dict)\n",
    "        probabilities = torch.nn.functional.softmax(outputs[0], dim=-1)\n",
    "        proba_list.append(probabilities)\n",
    "\n",
    "        start = end\n",
    "\n",
    "    return proba_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.3153, 0.1912, 0.4935]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.3153, 0.1912, 0.4935]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.3153, 0.1912, 0.4935]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0490, 0.0577, 0.8933]], grad_fn=<SoftmaxBackward0>)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba_list = chunk_text_to_window_size_and_predict_proba(input_ids, attention_mask, total_len)\n",
    "proba_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3153, 0.1912, 0.4935]],\n",
       "\n",
       "        [[0.3153, 0.1912, 0.4935]],\n",
       "\n",
       "        [[0.3153, 0.1912, 0.4935]],\n",
       "\n",
       "        [[0.0490, 0.0577, 0.8933]]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacks = torch.stack(proba_list)\n",
    "stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = stacks.shape\n",
    "shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3153, 0.1912, 0.4935],\n",
       "        [0.3153, 0.1912, 0.4935],\n",
       "        [0.3153, 0.1912, 0.4935],\n",
       "        [0.0490, 0.0577, 0.8933]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.reshape(stacks, (shape[0], shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\Itachi\\lib\\site-packages\\torch\\_tensor.py:955: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    }
   ],
   "source": [
    "def get_mean_from_proba(proba_list):\n",
    "    with torch.no_grad():\n",
    "        stacks = torch.stack(proba_list)\n",
    "        stacks = stacks.resize(stacks.shape[0], stacks.shape[2])\n",
    "        mean = stacks.mean(dim=0)\n",
    "    return mean\n",
    "\n",
    "mean = get_mean_from_proba(proba_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(mean).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[1015, 1012, 3020,  ..., 2015, 1012, 1524]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.encode_plus(txt, add_special_tokens=False, return_tensors='pt')\n",
    "print(len(tokens))\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_chunks = tokens['input_ids'][0].split(510)\n",
    "attention_mask_chunks = tokens['attention_mask'][0].split(510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_ids_and_attention_mask_chunks():\n",
    "    chunksize = 512\n",
    "    input_ids_chunks = list(tokens['input_ids'][0].split(chunksize - 2))\n",
    "    attention_mask_chunks = list(tokens['attention_mask'][0].split(chunksize - 2))\n",
    "\n",
    "    for i in range(len(input_ids_chunks)):\n",
    "        input_ids_chunks[i] = torch.cat([\n",
    "            torch.tensor([101]), input_ids_chunks[i], torch.tensor([102])\n",
    "        ])\n",
    "\n",
    "        attention_mask_chunks[i] = torch.cat([\n",
    "            torch.tensor([1]), attention_mask_chunks[i], torch.tensor([1])\n",
    "        ])\n",
    "\n",
    "        pad_len = chunksize - input_ids_chunks[i].shape[0]\n",
    "\n",
    "        if pad_len > 0:\n",
    "            input_ids_chunks[i] = torch.cat([\n",
    "                input_ids_chunks[i], torch.Tensor([0] * pad_len)\n",
    "            ])\n",
    "            attention_mask_chunks[i] = torch.cat([\n",
    "                attention_mask_chunks[i], torch.Tensor([0] * pad_len)\n",
    "            ])\n",
    "\n",
    "    return input_ids_chunks, attention_mask_chunks\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_chunks, attention_mask_chunks = get_input_ids_and_attention_mask_chunks()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1015,  1012,  ...,  1010,  1996,   102],\n",
       "         [  101, 10819,  3728,  ...,  1521,  2128,   102],\n",
       "         [  101,  2183,  2000,  ...,  2078,  5838,   102],\n",
       "         [  101,  1010, 23293,  ...,     0,     0,     0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]], dtype=torch.int32)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.stack(input_ids_chunks)\n",
    "attention_mask = torch.stack(attention_mask_chunks)\n",
    "\n",
    "input_dict = {\n",
    "    'input_ids' : input_ids.long(),\n",
    "    'attention_mask' : attention_mask.int()\n",
    "}\n",
    "\n",
    "input_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0734, 0.1197, 0.8068], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(**input_dict)\n",
    "\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=-1)\n",
    "\n",
    "mean_probabilities = probabilities.mean(dim=0)\n",
    "\n",
    "mean_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Itachi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
